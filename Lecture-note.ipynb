{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.0 Hello world\n",
    "\n",
    "print(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.11 Return Recap\n",
    "\n",
    "my_name = 'nico'\n",
    "my_age = 12\n",
    "print(f\"Hello I'm {my_name}, I'm {my_age} years old.\")\n",
    "\n",
    "def make_juice(fruit):\n",
    "    return f\"{fruit}+ğŸ¥¤\"\n",
    "\n",
    "def add_ice(juice):\n",
    "    return f\"{juice}+ğŸ§Š\"\n",
    "\n",
    "def add_sugar(iced_juice):\n",
    "    return f\"{iced_juice}+ğŸ¬\"\n",
    "\n",
    "juice = make_juice(\"ğŸ\")\n",
    "cold_juice = add_ice(juice)\n",
    "perfect_juice = add_sugar(cold_juice)\n",
    "\n",
    "print(perfect_juice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 And Or\n",
    "\n",
    "age = int(input(\"How old are you?\"))\n",
    "\n",
    "if age < 18:\n",
    "    print(\"You can't drink.\")\n",
    "\n",
    "elif age >= 18 and age <= 35:\n",
    "    print(\"You drink beer!\")\n",
    "\n",
    "elif age == 60 or age == 70:\n",
    "    print(\"Birthday, party!\")\n",
    "\n",
    "else:\n",
    "    print(\"Go ahead.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Python Standard Library\n",
    "\n",
    "import random\n",
    "\n",
    "user_choice = int(input(\"Choose number.\"))\n",
    "pc_choice = random.randint(1, 50)\n",
    "\n",
    "if user_choice == pc_choice:\n",
    "    print(\"You won!\")\n",
    "elif user_choice > pc_choice:\n",
    "    print(\"Lower! Computer chose\", pc_choice)\n",
    "elif user_choice < pc_choice:\n",
    "    print(\"Higher! Computer chose\", pc_choice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.6 Python Casino\n",
    "\n",
    "import random\n",
    "\n",
    "print(\"Welcome to Python Casino\")\n",
    "pc_choice = random.randint(1, 50)\n",
    "playing = True\n",
    "while playing:\n",
    "    user_choice = int(input(\"Choose number.\"))\n",
    "    if user_choice == pc_choice:\n",
    "        print(\"You won!\")\n",
    "        playing = False\n",
    "    elif user_choice > pc_choice:\n",
    "        print(\"Lower! Computer chose\", pc_choice)\n",
    "    elif user_choice < pc_choice:\n",
    "        print(\"Higher! Computer chose\", pc_choice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 For Loops\n",
    "\n",
    "websites = (\n",
    "    \"google.com\",\n",
    "    \"airbnb.com\",\n",
    "    \"twitter.com\",\n",
    "    \"facebook.com\"\n",
    ")\n",
    "\n",
    "for potato in websites:\n",
    "    print(\"potato is equalts to\", potato)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6 URL Formatting\n",
    "\n",
    "websites = (\n",
    "    \"google.com\",\n",
    "    \"airbnb.com\",\n",
    "    \"https://twitter.com\",\n",
    "    \"facebook.com\",\n",
    "    \"https://tiktok.com\"\n",
    ")\n",
    "\n",
    "for website in websites:\n",
    "    if not website.startswith(\"https://\"):\n",
    "        website = f\"https://{website}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.7 Requests\n",
    "\n",
    "from random import randint\n",
    "import requests\n",
    "\n",
    "websites = (\n",
    "    \"google.com\",\n",
    "    \"airbnb.com\",\n",
    "    \"https://twitter.com\",\n",
    "    \"facebook.com\",\n",
    "    \"https://tiktok.com\"\n",
    ")\n",
    "\n",
    "for website in websites:\n",
    "    if not website.startswith(\"https://\"):\n",
    "        website = f\"https://{website}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.8 Status Codes\n",
    "\n",
    "from random import randint\n",
    "import requests\n",
    "\n",
    "websites = (\n",
    "    \"google.com\",\n",
    "    \"airbnb.com\",\n",
    "    \"https://twitter.com\",\n",
    "    \"facebook.com\",\n",
    "    \"https://tiktok.com\"\n",
    ")\n",
    "\n",
    "result = {}\n",
    "\n",
    "for website in websites:\n",
    "    if not website.startswith(\"https://\"):\n",
    "        website = f\"https://{website}\"\n",
    "    response = requests.get(website)\n",
    "    if response.status_code == 200: # response ì •ìƒì¸ì§€ ê²€ì‚¬\n",
    "        print(f\"{website} is OK\")\n",
    "        result[website] = \"OK\"\n",
    "    else:\n",
    "        print(f\"{website} not OK\")\n",
    "        result[website] = \"FAILED\"\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Initial Request\n",
    "\n",
    "from requests import get\n",
    "\n",
    "base_url = \"https://weworkremotely.com/remote-jobs/search?term=\"\n",
    "search_term = \"python\"\n",
    "\n",
    "response = get(f\"{base_url}{search_term}\")\n",
    "if response.status_code != 200: # 200 = ì •ìƒ\n",
    "    print(\"Can't request website\")\n",
    "else:\n",
    "    print(response.text) # ì›¹ ì‚¬ì´íŠ¸ êµ¬ì„± HTML ì½”ë“œ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 BeautifulSoup\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = \"https://weworkremotely.com/remote-jobs/search?term=\"\n",
    "search_term = \"python\"\n",
    "\n",
    "response = get(f\"{base_url}{search_term}\")\n",
    "if response.status_code != 200: # 200 = ì •ìƒ\n",
    "    print(\"Can't request website\")\n",
    "else:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    jobs = soup.find_all('section', class_=\"jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.6 Job Posts\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = \"https://weworkremotely.com/remote-jobs/search?term=\"\n",
    "search_term = \"python\"\n",
    "\n",
    "response = get(f\"{base_url}{search_term}\")\n",
    "if response.status_code != 200: # 200 = ì •ìƒ\n",
    "    print(\"Can't request website\")\n",
    "else:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    jobs = soup.find_all('section', class_=\"jobs\") # <section> tag í™•ì¸\n",
    "    for job_section in jobs:\n",
    "        # job_posts ëŠ” <li> list\n",
    "        job_posts = job_section.find_all('li') # <section> ë‚´ <li> íƒœê·¸ í™•ì¸\n",
    "        job_posts.pop(-1) # ë§ˆì§€ë§‰ <li, class='view-all'> ì œê±°, í•„ìš” ì—†ìŒ\n",
    "        for post in job_posts:\n",
    "            print(post)\n",
    "            print(\"//////////////\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.7 Job Extraction\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = \"https://weworkremotely.com/remote-jobs/search?term=\"\n",
    "search_term = \"python\"\n",
    "\n",
    "response = get(f\"{base_url}{search_term}\")\n",
    "if response.status_code != 200: # 200 = ì •ìƒ\n",
    "    print(\"Can't request website\")\n",
    "else:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    jobs = soup.find_all('section', class_=\"jobs\") # <section> tag í™•ì¸\n",
    "    for job_section in jobs:\n",
    "        # job_posts ëŠ” <li> list\n",
    "        job_posts = job_section.find_all('li') # <section> ë‚´ <li> íƒœê·¸ í™•ì¸\n",
    "        job_posts.pop(-1) # ë§ˆì§€ë§‰ <li, class='view-all'> ì œê±°, í•„ìš” ì—†ìŒ\n",
    "        for post in job_posts:\n",
    "            anchors = post.find_all('a')\n",
    "            anchor = anchors[1] # anchorëŠ” 2ë²ˆì§¸êº¼ê°€ í•„ìš”\n",
    "            link = anchor['href']\n",
    "            # <a> ì•ˆì— <span> ì •ë³´ ì¶”ì¶œ: company, kind, region\n",
    "            company, kind, region = anchor.find_all('span', class_=\"company\")\n",
    "            title = anchor.find('span', class_='title')\n",
    "            print(company, kind, region, title)\n",
    "            print(\"///////////\")\n",
    "            print(\"///////////\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.8 Saving Results\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = \"https://weworkremotely.com/remote-jobs/search?term=\"\n",
    "search_term = \"python\"\n",
    "\n",
    "response = get(f\"{base_url}{search_term}\")\n",
    "if response.status_code != 200: # 200 = ì •ìƒ\n",
    "    print(\"Can't request website\")\n",
    "else:\n",
    "    results = []\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    jobs = soup.find_all('section', class_=\"jobs\") # <section> tag í™•ì¸\n",
    "    for job_section in jobs:\n",
    "        # job_posts ëŠ” <li> list\n",
    "        job_posts = job_section.find_all('li') # <section> ë‚´ <li> íƒœê·¸ í™•ì¸\n",
    "        job_posts.pop(-1) # ë§ˆì§€ë§‰ <li, class='view-all'> ì œê±°, í•„ìš” ì—†ìŒ\n",
    "        for post in job_posts:\n",
    "            anchors = post.find_all('a')\n",
    "            anchor = anchors[1] # anchorëŠ” 2ë²ˆì§¸êº¼ê°€ í•„ìš”\n",
    "            link = anchor['href']\n",
    "            # <a> ì•ˆì— <span> ì •ë³´ ì¶”ì¶œ: company, kind, region\n",
    "            company, kind, region = anchor.find_all('span', class_=\"company\")\n",
    "            title = anchor.find('span', class_='title')\n",
    "            job_data = {\n",
    "                'link': f\"https://weworkremotely.com{link}\",\n",
    "                'company': company.string,\n",
    "                'region': region.string,\n",
    "                'position': title.string\n",
    "            }\n",
    "            results.append(job_data)\n",
    "    for result in results:\n",
    "        print(result)\n",
    "        print(\"////////\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.10 Refactor\n",
    "\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_wwr_jobs(keyword):\n",
    "    base_url = \"https://weworkremotely.com/remote-jobs/search?term=\"\n",
    "\n",
    "    response = get(f\"{base_url}{keyword}\")\n",
    "    if response.status_code != 200: # 200 = ì •ìƒ\n",
    "        print(\"Can't request website\")\n",
    "    else:\n",
    "        results = []\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        jobs = soup.find_all('section', class_=\"jobs\") # <section> tag í™•ì¸\n",
    "        for job_section in jobs:\n",
    "            # job_posts ëŠ” <li> list\n",
    "            job_posts = job_section.find_all('li') # <section> ë‚´ <li> íƒœê·¸ í™•ì¸\n",
    "            job_posts.pop(-1) # ë§ˆì§€ë§‰ <li, class='view-all'> ì œê±°, í•„ìš” ì—†ìŒ\n",
    "            for post in job_posts:\n",
    "                anchors = post.find_all('a')\n",
    "                anchor = anchors[1] # anchorëŠ” 2ë²ˆì§¸êº¼ê°€ í•„ìš”\n",
    "                link = anchor['href']\n",
    "                # <a> ì•ˆì— <span> ì •ë³´ ì¶”ì¶œ: company, kind, region\n",
    "                company, kind, region = anchor.find_all('span', class_=\"company\")\n",
    "                title = anchor.find('span', class_='title')\n",
    "                job_data = {\n",
    "                    'link': f\"https://weworkremotely.com{link}\",\n",
    "                    'company': company.string,\n",
    "                    'region': region.string,\n",
    "                    'position': title.string\n",
    "                }\n",
    "                results.append(job_data)\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.11 Recursive\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from extractors.wwr import extract_wwr_jobs  # wwr = weworkremotely\n",
    "\n",
    "jobs = extract_wwr_jobs(\"python\")\n",
    "base_url = \"https://kr.indeed.com/jobs?q=\"\n",
    "search_term = \"python\"\n",
    "\n",
    "response = get(f\"{base_url}{search_term}\")\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(\"Can't request page\")\n",
    "else:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    job_list = soup.find(\"ul\", class_=\"jobsearch-ResultsList\")\n",
    "    job = job_list.find_all('li', recursive=False)\n",
    "    for job in jobs:\n",
    "        print(job)\n",
    "        print(\"///////\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.12 Indeed 403 Fix\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from extractors.wwr import extract_wwr_jobs\n",
    "from selenium import webdriver\n",
    "\n",
    "# from selenium.webdriver.chrome.options import Options \n",
    "# >>VSCí™˜ê²½ì—ì„œëŠ” ì˜µì…˜ ë³€ê²½ í•„ìš”ì—†ìŒ\n",
    "\n",
    "# options = Options()\n",
    "# options.add_argument(\"--no-sandbox\")\n",
    "# options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# browser = webdriver.Chrome(options=options)\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "browser.get(\"https://kr.indeed.com/jobs?q=python\")\n",
    "\n",
    "print(browser.page_source)\n",
    "\n",
    "# ì½”ë“œ ë¬´í•œëŒ€ê¸°: í¬ë¡¬ ë“œë¼ì´ë²„ ë²„ì „ ì•ˆë§ì•„ì„œ ìƒê¸°ëŠ” ë¬¸ì œ!!\n",
    "while(True):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.13 None\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# options = Options()\n",
    "# options.add_argument(\"--no-sandbox\")\n",
    "# options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# browser = webdriver.Chrome(options=options)\n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "browser.get(\"https://kr.indeed.com/jobs?q=python\")\n",
    "\n",
    "soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "job_list = soup.find(\"ul\", class_=\"jobsearch-ResultsList\")\n",
    "\n",
    "\n",
    "jobs = job_list.find_all('li', recursive=False)\n",
    "\n",
    "for job in jobs:\n",
    "    zone = job.find(\"div\", class_=\"mosaic-zone\")\n",
    "    if zone == None: # mosaic-zoneì„ ì°¾ì§€ ëª»í•˜ë©´, None ë¦¬í„´\n",
    "        print(\"job li\")\n",
    "    else:\n",
    "        print(\"mosaic li\")\n",
    "\n",
    "# ì½”ë“œ ë¬´í•œëŒ€ê¸°: í¬ë¡¬ ë“œë¼ì´ë²„ ë²„ì „ ì•ˆë§ì•„ì„œ ìƒê¸°ëŠ” ë¬¸ì œ!!\n",
    "while (True):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.14 Select\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# options = Options()\n",
    "# options.add_argument(\"--no-sandbox\")\n",
    "# options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# browser = webdriver.Chrome(options=options)\n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "browser.get(\"https://kr.indeed.com/jobs?q=python\")\n",
    "results = []\n",
    "soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "job_list = soup.find(\"ul\", class_=\"jobsearch-ResultsList\")\n",
    "\n",
    "\n",
    "jobs = job_list.find_all('li', recursive=False)\n",
    "\n",
    "for job in jobs:\n",
    "    zone = job.find(\"div\", class_=\"mosaic-zone\")\n",
    "    if zone == None: # mosaic-zoneì„ ì°¾ì§€ ëª»í•˜ë©´, None ë¦¬í„´\n",
    "        anchor = job.select_one(\"h2 a\")\n",
    "        title = anchor['aria-label']\n",
    "        link = anchor['href']\n",
    "        company = job.find(\"span\", class_=\"companyName\")\n",
    "        location = job.find(\"div\", class_=\"companyLocation\")\n",
    "        job_data = {\n",
    "            'link': f\"https://kr.indeed.com{link}\",\n",
    "            'company': company.string,\n",
    "            'location': location.string,\n",
    "            'position': title\n",
    "        }\n",
    "        results.append(job_data)\n",
    "for result in results:\n",
    "    print(result, \"\\n///////\\n\")        \n",
    "# ì½”ë“œ ë¬´í•œëŒ€ê¸°: í¬ë¡¬ ë“œë¼ì´ë²„ ë²„ì „ ì•ˆë§ì•„ì„œ ìƒê¸°ëŠ” ë¬¸ì œ!!\n",
    "while (True):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.15 Pages\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# options = Options()\n",
    "# options.add_argument(\"--no-sandbox\")\n",
    "# options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# browser = webdriver.Chrome(options=options)\n",
    "\n",
    "def get_page_count(keyword):\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(f\"https://kr.indeed.com/jobs?q={keyword}\")\n",
    "    soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "    pagination = soup.find(\"ul\", class_=\"pagination-list\") # ì²« í˜ì´ì§€ í™•ì¸\n",
    "    if pagination == None: # pagination = Noneì´ë©´ ìŠ¤í¬ë˜í•‘í•  í˜ì´ì§€ 1ê°œ\n",
    "        return 1\n",
    "    pages = pagination.find_all(\"li\", recursive=False) # í˜ì´ì§€ 1, 2, 3, 4, 5, ë‹¤ìŒ -> len = 6\n",
    "    count = len(pages)\n",
    "    return count\n",
    "\n",
    "print(get_page_count(\"python\"))\n",
    "\n",
    "def extract_indeed_jobs(keyword):\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(f\"https://kr.indeed.com/jobs?q={keyword}\")\n",
    "    results = []\n",
    "    soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "    job_list = soup.find(\"ul\", class_=\"jobsearch-ResultsList\")\n",
    "\n",
    "    jobs = job_list.find_all('li', recursive=False)\n",
    "\n",
    "    for job in jobs:\n",
    "        zone = job.find(\"div\", class_=\"mosaic-zone\")\n",
    "        if zone == None: # mosaic-zoneì„ ì°¾ì§€ ëª»í•˜ë©´, None ë¦¬í„´\n",
    "            anchor = job.select_one(\"h2 a\")\n",
    "            title = anchor['aria-label']\n",
    "            link = anchor['href']\n",
    "            company = job.find(\"span\", class_=\"companyName\")\n",
    "            location = job.find(\"div\", class_=\"companyLocation\")\n",
    "            job_data = {\n",
    "                'link': f\"https://kr.indeed.com{link}\",\n",
    "                'company': company.string,\n",
    "                'location': location.string,\n",
    "                'position': title\n",
    "            }\n",
    "            results.append(job_data)\n",
    "    for result in results:\n",
    "        print(result, \"\\n///////\\n\")        \n",
    "\n",
    "# ì½”ë“œ ë¬´í•œëŒ€ê¸°: í¬ë¡¬ ë“œë¼ì´ë²„ ë²„ì „ ì•ˆë§ì•„ì„œ ìƒê¸°ëŠ” ë¬¸ì œ!!\n",
    "while (True):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.16 Pages Part Two\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# options = Options()\n",
    "# options.add_argument(\"--no-sandbox\")\n",
    "# options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# browser = webdriver.Chrome(options=options)\n",
    "\n",
    "def get_page_count(keyword):\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(f\"https://kr.indeed.com/jobs?q={keyword}\")\n",
    "    soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "    pagination = soup.find(\"ul\", class_=\"pagination-list\") # ì²« í˜ì´ì§€ í™•ì¸\n",
    "    if pagination == None: # pagination = Noneì´ë©´ ìŠ¤í¬ë˜í•‘í•  í˜ì´ì§€ 1ê°œ\n",
    "        return 1\n",
    "    pages = pagination.find_all(\"li\", recursive=False) # í˜ì´ì§€ 1, 2, 3, 4, 5, ë‹¤ìŒ -> len = 6\n",
    "    count = len(pages)\n",
    "    if count >= 5:\n",
    "        return 5\n",
    "    else:\n",
    "        return count\n",
    "\n",
    "\n",
    "def extract_indeed_jobs(keyword):\n",
    "    pages = get_page_count(keyword)\n",
    "    for page in range(pages):\n",
    "        browser = webdriver.Chrome()\n",
    "        browser.get(f\"https://kr.indeed.com/jobs?q={keyword}\")\n",
    "        results = []\n",
    "        soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "        job_list = soup.find(\"ul\", class_=\"jobsearch-ResultsList\")\n",
    "\n",
    "        jobs = job_list.find_all('li', recursive=False)\n",
    "\n",
    "        for job in jobs:\n",
    "            zone = job.find(\"div\", class_=\"mosaic-zone\")\n",
    "            if zone == None: # mosaic-zoneì„ ì°¾ì§€ ëª»í•˜ë©´, None ë¦¬í„´\n",
    "                anchor = job.select_one(\"h2 a\")\n",
    "                title = anchor['aria-label']\n",
    "                link = anchor['href']\n",
    "                company = job.find(\"span\", class_=\"companyName\")\n",
    "                location = job.find(\"div\", class_=\"companyLocation\")\n",
    "                job_data = {\n",
    "                    'link': f\"https://kr.indeed.com{link}\",\n",
    "                    'company': company.string,\n",
    "                    'location': location.string,\n",
    "                    'position': title\n",
    "                }\n",
    "                results.append(job_data)\n",
    "   \n",
    "# ì½”ë“œ ë¬´í•œëŒ€ê¸°: í¬ë¡¬ ë“œë¼ì´ë²„ ë²„ì „ ì•ˆë§ì•„ì„œ ìƒê¸°ëŠ” ë¬¸ì œ!!\n",
    "while (True):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.17 Refactor\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from extractors.wwr import extract_wwr_jobs\n",
    "# bot ì¸ì¤„ ì•Œê³  ì°¨ë‹¨ë‹¹í•´ì„œ selenium ì¨ì•¼í•¨\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "def get_page_count(keyword):\n",
    "    options = Options()\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    browser = webdriver.Chrome(options = options)\n",
    "    base_url = \"https://kr.indeed.com/jobs?q=\"\n",
    "    browser.get(f\"{base_url}{keyword}\")\n",
    "    soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "    # navë¡œ HTML ë°”ê¼‡ìŒ, + aria-label ì¶”ê°€\n",
    "    pagination = soup.find(\"nav\", {\"aria-label\":\"pagination\"})\n",
    "    if pagination == None:\n",
    "        return 1\n",
    "    else:\n",
    "        pages = pagination.find_all(\"div\", recursive = False)\n",
    "        count = len(pages)\n",
    "    if count >= 5:\n",
    "        return 5\n",
    "    else:\n",
    "        return count\n",
    "\n",
    "def extract_indeed_jobs(keyword):\n",
    "    pages = get_page_count(keyword)\n",
    "    print(\"found\", pages)\n",
    "    # page ëŠ” 0 ë¶€í„° ì‹œì‘\n",
    "    results = []\n",
    "    for page in range(pages):\n",
    "        # Replit ì˜µì…˜, VSC ì—ì„œëŠ” í•„ìš” X\n",
    "        options = Options()\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        browser = webdriver.Chrome(options = options)\n",
    "\n",
    "        base_url = \"https://kr.indeed.com/jobs\"\n",
    "        final_url = f\"{base_url}?q={keyword}&start={page*10}\"\n",
    "        browser.get(final_url)\n",
    "        print(final_url)\n",
    "\n",
    "        soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "        job_lists = soup.find(\"ul\", class_ = \"jobsearch-ResultsList\")\n",
    "        jobs = job_lists.find_all(\"li\", recursive = False)\n",
    "        # recursive ë¥¼ False ë¡œ í•˜ë©´ ì•„ë«ë‹¨ê³„ê¹Œì§€ ì°¾ëŠ”ê²Œ ì•„ë‹ˆë¼ ë°”ë¡œ ì•„ë˜ ë‹¨ê³„ì—ì„œë§Œ ì°¾ëŠ”ë‹¤\n",
    "        for job in jobs:\n",
    "            zone = job.find(\"div\", class_ = \"mosaic-zone\")\n",
    "            if zone == None: # None ì€ ì—†ìŒì„ ì˜ë¯¸, False ë‘ì€ ë‹¤ë¦„\n",
    "                #anchor = job.select(\"h2 a\") #h2 ì•ˆì— ìˆëŠ” a ë¥¼ ê°€ì ¸ì˜¤ê¸°\n",
    "                anchor = job.select_one(\"h2 a\")\n",
    "                title = anchor['aria-label']\n",
    "                link = anchor['href']\n",
    "                company = job.find(\"span\", class_ = \"companyName\")\n",
    "                region = job.find(\"div\", class_ = \"companyLocation\")\n",
    "            job_data = {\n",
    "                \"link\": f\"https://kr.indeed.com/{link}\",\n",
    "                \"company\": company.string,\n",
    "                \"location\": region.sting,\n",
    "                \"position\": title\n",
    "                }\n",
    "            results.append(job_data)\n",
    "    return results\n",
    "\n",
    "num = get_page_count(\"react\")\n",
    "#result = extract_indeed_jobs(\"python\")\n",
    "print(num)\n",
    "print(extract_indeed_jobs(\"react\"))\n",
    "\n",
    "# ì½”ë“œ ë¬´í•œëŒ€ê¸°: í¬ë¡¬ ë“œë¼ì´ë²„ ë²„ì „ ì•ˆë§ì•„ì„œ ìƒê¸°ëŠ” ë¬¸ì œ!!\n",
    "#while (True):\n",
    "#    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.18 Recap\n",
    "\n",
    "from extractors.indeed import extract_indeed_jobs\n",
    "from extractors.wwr import extract_wwr_jobs\n",
    "\n",
    "keyword = input(\"What do you want to search for?\")\n",
    "\n",
    "# indeed, wwr: list ë¡œ ë¦¬í„´\n",
    "indeed = extract_indeed_jobs(keyword)\n",
    "wwr = extract_wwr_jobs(keyword)\n",
    "\n",
    "jobs = indeed + wwr\n",
    "\n",
    "for job in jobs:\n",
    "    print(job)\n",
    "    print(\"/////\\n/////\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.19 Write to File\n",
    "\n",
    "from extractors.indeed import extract_indeed_jobs\n",
    "from extractors.wwr import extract_wwr_jobs\n",
    "\n",
    "keyword = input(\"What do you want to search for?\")\n",
    "\n",
    "# indeed, wwr: list ë¡œ ë¦¬í„´\n",
    "indeed = extract_indeed_jobs(keyword)\n",
    "wwr = extract_wwr_jobs(keyword)\n",
    "\n",
    "jobs = indeed + wwr\n",
    "\n",
    "file = open(f\"{keyword}.csv\", \"w\") # csv íŒŒì¼ ìƒì„±\n",
    "file.write(\"Position, Company, Location, URL\\n\") # Column ì´ë¦„\n",
    "\n",
    "for job in jobs: # ê° dict ë¶ˆëŸ¬ì™€ csvì— ì‘ì„±\n",
    "    file.write(f\"{job['position']}, {job['company']}, {job['location']}, {job['link']}\\n\")\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Hello Flask\n",
    "\n",
    "from flask import Flask\n",
    "\n",
    "app = Flask(\"JobScrapper\")\n",
    "\n",
    "\n",
    "@app.route(\"/\") # ì´ ìœ„ì¹˜ë¡œ ì´ë™ì‹œ, ì•„ë˜ í•¨ìˆ˜ ì‹¤í–‰\n",
    "def home():\n",
    "    return \"hey there!\"\n",
    "\n",
    "\n",
    "app.run(\"0.0.0.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Render Template\n",
    "\n",
    "from flask import Flask\n",
    "\n",
    "app = Flask(\"JobScrapper\")\n",
    "\n",
    "\n",
    "@app.route(\"/\") # ì´ ìœ„ì¹˜ë¡œ ì´ë™ì‹œ, ì•„ë˜ í•¨ìˆ˜ ì‹¤í–‰\n",
    "def home():\n",
    "    return \"<h1>hey there!</h1>\"\n",
    "\n",
    "@app.route(\"/hello\")\n",
    "def hello():\n",
    "    return 'hello you!'\n",
    "\n",
    "app.run(\"0.0.0.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.5 Arguments\n",
    "\n",
    "from flask import Flask, render_template, request\n",
    "from extractors.indeed import extract_indeed_jobs\n",
    "from extractors.wwr import extract_wwr_jobs\n",
    "\n",
    "app = Flask(\"JobScrapper\")\n",
    "\n",
    "\n",
    "@app.route(\"/\") # ì´ ìœ„ì¹˜ë¡œ ì´ë™ì‹œ, ì•„ë˜ í•¨ìˆ˜ ì‹¤í–‰\n",
    "def home():\n",
    "    return render_template(\"home.html\", name=\"nico\")\n",
    "\n",
    "@app.route(\"/search\")\n",
    "def search():\n",
    "    keyword = request.args.get(\"keyword\")\n",
    "    indeed = extract_indeed_jobs(keyword)\n",
    "    wwr = extract_wwr_jobs(keyword)\n",
    "    jobs = indeed + wwr\n",
    "    return render_template(\"search.html\", keyword=keyword, jobs=jobs)\n",
    "\n",
    "app.run(\"0.0.0.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.7 Pico\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Job Scrapper</title>\n",
    "    <link rel=\"stylesheet\" href=\"https://unpkg.com/@picocss/pico@latest/css/pico.min.css\">\n",
    "</head>\n",
    "<body>\n",
    "    <main class=\"container\">\n",
    "        <h1>Search Result for \"{{keyword}}\":</h1>\n",
    "        <figure>\n",
    "            <table role=\"grid\">\n",
    "                <thead>\n",
    "                    <tr>\n",
    "                        <th>Position</th>\n",
    "                        <th>Company</th>\n",
    "                        <th>Location</th>\n",
    "                        <th>Link</th>\n",
    "                    </tr>\n",
    "                </thead>\n",
    "                <tbody>\n",
    "                    {% for job in jobs %}\n",
    "                    <tr>\n",
    "                        <td>{{job.position}}</td>\n",
    "                        <td>{{job.company}}</td>\n",
    "                        <td>{{job.location}}</td>\n",
    "                        <td><a href=\"{{job.link}}\" target=\"_blank\">Apply &rarr;</a></td>\n",
    "                    </tr>\n",
    "                    {% endfor %}\n",
    "                </tbody>\n",
    "            </table>\n",
    "        </figure>\n",
    "    </main>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.8 Cache\n",
    "\n",
    "from flask import Flask, render_template, request\n",
    "from extractors.indeed import extract_indeed_jobs\n",
    "from extractors.wwr import extract_wwr_jobs\n",
    "\n",
    "app = Flask(\"JobScrapper\")\n",
    "\n",
    "\n",
    "@app.route(\"/\") # ì´ ìœ„ì¹˜ë¡œ ì´ë™ì‹œ, ì•„ë˜ í•¨ìˆ˜ ì‹¤í–‰\n",
    "def home():\n",
    "    return render_template(\"home.html\", name=\"nico\")\n",
    "\n",
    "db = {} # ê°€ì§œ DB, Cache\n",
    "\n",
    "@app.route(\"/search\")\n",
    "def search():\n",
    "    keyword = request.args.get(\"keyword\")\n",
    "    if keyword in db:\n",
    "        jobs = db[keyword]\n",
    "    else:\n",
    "        indeed = extract_indeed_jobs(keyword)\n",
    "        wwr = extract_wwr_jobs(keyword)\n",
    "        jobs = indeed + wwr\n",
    "        db[keyword] = jobs\n",
    "    return render_template(\"search.html\", keyword=keyword, jobs=jobs)\n",
    "\n",
    "app.run(\"0.0.0.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.9 File Download\n",
    "\n",
    "from flask import Flask, render_template, request, redirect, send_file\n",
    "from extractors.indeed import extract_indeed_jobs\n",
    "from extractors.wwr import extract_wwr_jobs\n",
    "from file import save_to_file\n",
    "\n",
    "app = Flask(\"JobScrapper\")\n",
    "\n",
    "\n",
    "@app.route(\"/\") # ì´ ìœ„ì¹˜ë¡œ ì´ë™ì‹œ, ì•„ë˜ í•¨ìˆ˜ ì‹¤í–‰\n",
    "def home():\n",
    "    return render_template(\"home.html\", name=\"nico\")\n",
    "\n",
    "db = {} # ê°€ì§œ DB, Cache\n",
    "\n",
    "@app.route(\"/search\")\n",
    "def search():\n",
    "    keyword = request.args.get(\"keyword\")\n",
    "    if keyword == None:\n",
    "        return redirect(\"/\")\n",
    "    if keyword in db:\n",
    "        jobs = db[keyword]\n",
    "    else:\n",
    "        indeed = extract_indeed_jobs(keyword)\n",
    "        wwr = extract_wwr_jobs(keyword)\n",
    "        jobs = indeed + wwr\n",
    "        db[keyword] = jobs\n",
    "    return render_template(\"search.html\", keyword=keyword, jobs=jobs)\n",
    "\n",
    "@app.route(\"/export\")\n",
    "def export():\n",
    "    keyword = request.args.get(\"keyword\")\n",
    "    if keyword == None:\n",
    "        return redirect(\"/\")\n",
    "    if keyword not in db:\n",
    "        return redirect(f\"/search?keyword={keyword}\")\n",
    "    save_to_file(keyword, db[keyword])\n",
    "    return send_file(f\"{keyword}.csv\", as_attachment=True)\n",
    "\n",
    "app.run(\"0.0.0.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
