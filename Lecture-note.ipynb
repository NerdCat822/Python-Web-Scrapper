{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.0 Hello world\n",
    "\n",
    "print(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.11 Return Recap\n",
    "\n",
    "my_name = 'nico'\n",
    "my_age = 12\n",
    "print(f\"Hello I'm {my_name}, I'm {my_age} years old.\")\n",
    "\n",
    "def make_juice(fruit):\n",
    "    return f\"{fruit}+🥤\"\n",
    "\n",
    "def add_ice(juice):\n",
    "    return f\"{juice}+🧊\"\n",
    "\n",
    "def add_sugar(iced_juice):\n",
    "    return f\"{iced_juice}+🍬\"\n",
    "\n",
    "juice = make_juice(\"🍎\")\n",
    "cold_juice = add_ice(juice)\n",
    "perfect_juice = add_sugar(cold_juice)\n",
    "\n",
    "print(perfect_juice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 And Or\n",
    "\n",
    "age = int(input(\"How old are you?\"))\n",
    "\n",
    "if age < 18:\n",
    "    print(\"You can't drink.\")\n",
    "\n",
    "elif age >= 18 and age <= 35:\n",
    "    print(\"You drink beer!\")\n",
    "\n",
    "elif age == 60 or age == 70:\n",
    "    print(\"Birthday, party!\")\n",
    "\n",
    "else:\n",
    "    print(\"Go ahead.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Python Standard Library\n",
    "\n",
    "import random\n",
    "\n",
    "user_choice = int(input(\"Choose number.\"))\n",
    "pc_choice = random.randint(1, 50)\n",
    "\n",
    "if user_choice == pc_choice:\n",
    "    print(\"You won!\")\n",
    "elif user_choice > pc_choice:\n",
    "    print(\"Lower! Computer chose\", pc_choice)\n",
    "elif user_choice < pc_choice:\n",
    "    print(\"Higher! Computer chose\", pc_choice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.6 Python Casino\n",
    "\n",
    "import random\n",
    "\n",
    "print(\"Welcome to Python Casino\")\n",
    "pc_choice = random.randint(1, 50)\n",
    "playing = True\n",
    "while playing:\n",
    "    user_choice = int(input(\"Choose number.\"))\n",
    "    if user_choice == pc_choice:\n",
    "        print(\"You won!\")\n",
    "        playing = False\n",
    "    elif user_choice > pc_choice:\n",
    "        print(\"Lower! Computer chose\", pc_choice)\n",
    "    elif user_choice < pc_choice:\n",
    "        print(\"Higher! Computer chose\", pc_choice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 For Loops\n",
    "\n",
    "websites = (\n",
    "    \"google.com\",\n",
    "    \"airbnb.com\",\n",
    "    \"twitter.com\",\n",
    "    \"facebook.com\"\n",
    ")\n",
    "\n",
    "for potato in websites:\n",
    "    print(\"potato is equalts to\", potato)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6 URL Formatting\n",
    "\n",
    "websites = (\n",
    "    \"google.com\",\n",
    "    \"airbnb.com\",\n",
    "    \"https://twitter.com\",\n",
    "    \"facebook.com\",\n",
    "    \"https://tiktok.com\"\n",
    ")\n",
    "\n",
    "for website in websites:\n",
    "    if not website.startswith(\"https://\"):\n",
    "        website = f\"https://{website}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.7 Requests\n",
    "\n",
    "from random import randint\n",
    "import requests\n",
    "\n",
    "websites = (\n",
    "    \"google.com\",\n",
    "    \"airbnb.com\",\n",
    "    \"https://twitter.com\",\n",
    "    \"facebook.com\",\n",
    "    \"https://tiktok.com\"\n",
    ")\n",
    "\n",
    "for website in websites:\n",
    "    if not website.startswith(\"https://\"):\n",
    "        website = f\"https://{website}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.8 Status Codes\n",
    "\n",
    "from random import randint\n",
    "import requests\n",
    "\n",
    "websites = (\n",
    "    \"google.com\",\n",
    "    \"airbnb.com\",\n",
    "    \"https://twitter.com\",\n",
    "    \"facebook.com\",\n",
    "    \"https://tiktok.com\"\n",
    ")\n",
    "\n",
    "result = {}\n",
    "\n",
    "for website in websites:\n",
    "    if not website.startswith(\"https://\"):\n",
    "        website = f\"https://{website}\"\n",
    "    response = requests.get(website)\n",
    "    if response.status_code == 200: # response 정상인지 검사\n",
    "        print(f\"{website} is OK\")\n",
    "        result[website] = \"OK\"\n",
    "    else:\n",
    "        print(f\"{website} not OK\")\n",
    "        result[website] = \"FAILED\"\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Initial Request\n",
    "\n",
    "from requests import get\n",
    "\n",
    "base_url = \"https://weworkremotely.com/remote-jobs/search?term=\"\n",
    "search_term = \"python\"\n",
    "\n",
    "response = get(f\"{base_url}{search_term}\")\n",
    "if response.status_code != 200: # 200 = 정상\n",
    "    print(\"Can't request website\")\n",
    "else:\n",
    "    print(response.text) # 웹 사이트 구성 HTML 코드\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 BeautifulSoup\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = \"https://weworkremotely.com/remote-jobs/search?term=\"\n",
    "search_term = \"python\"\n",
    "\n",
    "response = get(f\"{base_url}{search_term}\")\n",
    "if response.status_code != 200: # 200 = 정상\n",
    "    print(\"Can't request website\")\n",
    "else:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    jobs = soup.find_all('section', class_=\"jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.6 Job Posts\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = \"https://weworkremotely.com/remote-jobs/search?term=\"\n",
    "search_term = \"python\"\n",
    "\n",
    "response = get(f\"{base_url}{search_term}\")\n",
    "if response.status_code != 200: # 200 = 정상\n",
    "    print(\"Can't request website\")\n",
    "else:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    jobs = soup.find_all('section', class_=\"jobs\") # <section> tag 확인\n",
    "    for job_section in jobs:\n",
    "        # job_posts 는 <li> list\n",
    "        job_posts = job_section.find_all('li') # <section> 내 <li> 태그 확인\n",
    "        job_posts.pop(-1) # 마지막 <li, class='view-all'> 제거, 필요 없음\n",
    "        for post in job_posts:\n",
    "            print(post)\n",
    "            print(\"//////////////\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.7 Job Extraction\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = \"https://weworkremotely.com/remote-jobs/search?term=\"\n",
    "search_term = \"python\"\n",
    "\n",
    "response = get(f\"{base_url}{search_term}\")\n",
    "if response.status_code != 200: # 200 = 정상\n",
    "    print(\"Can't request website\")\n",
    "else:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    jobs = soup.find_all('section', class_=\"jobs\") # <section> tag 확인\n",
    "    for job_section in jobs:\n",
    "        # job_posts 는 <li> list\n",
    "        job_posts = job_section.find_all('li') # <section> 내 <li> 태그 확인\n",
    "        job_posts.pop(-1) # 마지막 <li, class='view-all'> 제거, 필요 없음\n",
    "        for post in job_posts:\n",
    "            anchors = post.find_all('a')\n",
    "            anchor = anchors[1] # anchor는 2번째꺼가 필요\n",
    "            link = anchor['href']\n",
    "            # <a> 안에 <span> 정보 추출: company, kind, region\n",
    "            company, kind, region = anchor.find_all('span', class_=\"company\")\n",
    "            title = anchor.find('span', class_='title')\n",
    "            print(company, kind, region, title)\n",
    "            print(\"///////////\")\n",
    "            print(\"///////////\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.8 Saving Results\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = \"https://weworkremotely.com/remote-jobs/search?term=\"\n",
    "search_term = \"python\"\n",
    "\n",
    "response = get(f\"{base_url}{search_term}\")\n",
    "if response.status_code != 200: # 200 = 정상\n",
    "    print(\"Can't request website\")\n",
    "else:\n",
    "    results = []\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    jobs = soup.find_all('section', class_=\"jobs\") # <section> tag 확인\n",
    "    for job_section in jobs:\n",
    "        # job_posts 는 <li> list\n",
    "        job_posts = job_section.find_all('li') # <section> 내 <li> 태그 확인\n",
    "        job_posts.pop(-1) # 마지막 <li, class='view-all'> 제거, 필요 없음\n",
    "        for post in job_posts:\n",
    "            anchors = post.find_all('a')\n",
    "            anchor = anchors[1] # anchor는 2번째꺼가 필요\n",
    "            link = anchor['href']\n",
    "            # <a> 안에 <span> 정보 추출: company, kind, region\n",
    "            company, kind, region = anchor.find_all('span', class_=\"company\")\n",
    "            title = anchor.find('span', class_='title')\n",
    "            job_data = {\n",
    "                'link': f\"https://weworkremotely.com{link}\",\n",
    "                'company': company.string,\n",
    "                'region': region.string,\n",
    "                'position': title.string\n",
    "            }\n",
    "            results.append(job_data)\n",
    "    for result in results:\n",
    "        print(result)\n",
    "        print(\"////////\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.10 Refactor\n",
    "\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_wwr_jobs(keyword):\n",
    "    base_url = \"https://weworkremotely.com/remote-jobs/search?term=\"\n",
    "\n",
    "    response = get(f\"{base_url}{keyword}\")\n",
    "    if response.status_code != 200: # 200 = 정상\n",
    "        print(\"Can't request website\")\n",
    "    else:\n",
    "        results = []\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        jobs = soup.find_all('section', class_=\"jobs\") # <section> tag 확인\n",
    "        for job_section in jobs:\n",
    "            # job_posts 는 <li> list\n",
    "            job_posts = job_section.find_all('li') # <section> 내 <li> 태그 확인\n",
    "            job_posts.pop(-1) # 마지막 <li, class='view-all'> 제거, 필요 없음\n",
    "            for post in job_posts:\n",
    "                anchors = post.find_all('a')\n",
    "                anchor = anchors[1] # anchor는 2번째꺼가 필요\n",
    "                link = anchor['href']\n",
    "                # <a> 안에 <span> 정보 추출: company, kind, region\n",
    "                company, kind, region = anchor.find_all('span', class_=\"company\")\n",
    "                title = anchor.find('span', class_='title')\n",
    "                job_data = {\n",
    "                    'link': f\"https://weworkremotely.com{link}\",\n",
    "                    'company': company.string,\n",
    "                    'region': region.string,\n",
    "                    'position': title.string\n",
    "                }\n",
    "                results.append(job_data)\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.11 Recursive\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from extractors.wwr import extract_wwr_jobs  # wwr = weworkremotely\n",
    "\n",
    "jobs = extract_wwr_jobs(\"python\")\n",
    "base_url = \"https://kr.indeed.com/jobs?q=\"\n",
    "search_term = \"python\"\n",
    "\n",
    "response = get(f\"{base_url}{search_term}\")\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(\"Can't request page\")\n",
    "else:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    job_list = soup.find(\"ul\", class_=\"jobsearch-ResultsList\")\n",
    "    job = job_list.find_all('li', recursive=False)\n",
    "    for job in jobs:\n",
    "        print(job)\n",
    "        print(\"///////\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.12 Indeed 403 Fix\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from extractors.wwr import extract_wwr_jobs\n",
    "from selenium import webdriver\n",
    "\n",
    "# from selenium.webdriver.chrome.options import Options \n",
    "# >>VSC환경에서는 옵션 변경 필요없음\n",
    "\n",
    "# options = Options()\n",
    "# options.add_argument(\"--no-sandbox\")\n",
    "# options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# browser = webdriver.Chrome(options=options)\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "browser.get(\"https://kr.indeed.com/jobs?q=python\")\n",
    "\n",
    "print(browser.page_source)\n",
    "\n",
    "# 코드 무한대기: 크롬 드라이버 버전 안맞아서 생기는 문제!!\n",
    "while(True):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.13 None\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# options = Options()\n",
    "# options.add_argument(\"--no-sandbox\")\n",
    "# options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# browser = webdriver.Chrome(options=options)\n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "browser.get(\"https://kr.indeed.com/jobs?q=python\")\n",
    "\n",
    "soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "job_list = soup.find(\"ul\", class_=\"jobsearch-ResultsList\")\n",
    "\n",
    "\n",
    "jobs = job_list.find_all('li', recursive=False)\n",
    "\n",
    "for job in jobs:\n",
    "    zone = job.find(\"div\", class_=\"mosaic-zone\")\n",
    "    if zone == None: # mosaic-zone을 찾지 못하면, None 리턴\n",
    "        print(\"job li\")\n",
    "    else:\n",
    "        print(\"mosaic li\")\n",
    "\n",
    "# 코드 무한대기: 크롬 드라이버 버전 안맞아서 생기는 문제!!\n",
    "while (True):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.14 Select\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# options = Options()\n",
    "# options.add_argument(\"--no-sandbox\")\n",
    "# options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# browser = webdriver.Chrome(options=options)\n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "browser.get(\"https://kr.indeed.com/jobs?q=python\")\n",
    "results = []\n",
    "soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "job_list = soup.find(\"ul\", class_=\"jobsearch-ResultsList\")\n",
    "\n",
    "\n",
    "jobs = job_list.find_all('li', recursive=False)\n",
    "\n",
    "for job in jobs:\n",
    "    zone = job.find(\"div\", class_=\"mosaic-zone\")\n",
    "    if zone == None: # mosaic-zone을 찾지 못하면, None 리턴\n",
    "        anchor = job.select_one(\"h2 a\")\n",
    "        title = anchor['aria-label']\n",
    "        link = anchor['href']\n",
    "        company = job.find(\"span\", class_=\"companyName\")\n",
    "        location = job.find(\"div\", class_=\"companyLocation\")\n",
    "        job_data = {\n",
    "            'link': f\"https://kr.indeed.com{link}\",\n",
    "            'company': company.string,\n",
    "            'location': location.string,\n",
    "            'position': title\n",
    "        }\n",
    "        results.append(job_data)\n",
    "for result in results:\n",
    "    print(result, \"\\n///////\\n\")        \n",
    "# 코드 무한대기: 크롬 드라이버 버전 안맞아서 생기는 문제!!\n",
    "while (True):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.15 Pages\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# options = Options()\n",
    "# options.add_argument(\"--no-sandbox\")\n",
    "# options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# browser = webdriver.Chrome(options=options)\n",
    "\n",
    "def get_page_count(keyword):\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(f\"https://kr.indeed.com/jobs?q={keyword}\")\n",
    "    soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "    pagination = soup.find(\"ul\", class_=\"pagination-list\") # 첫 페이지 확인\n",
    "    if pagination == None: # pagination = None이면 스크래핑할 페이지 1개\n",
    "        return 1\n",
    "    pages = pagination.find_all(\"li\", recursive=False) # 페이지 1, 2, 3, 4, 5, 다음 -> len = 6\n",
    "    count = len(pages)\n",
    "    return count\n",
    "\n",
    "print(get_page_count(\"python\"))\n",
    "\n",
    "def extract_indeed_jobs(keyword):\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(f\"https://kr.indeed.com/jobs?q={keyword}\")\n",
    "    results = []\n",
    "    soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "    job_list = soup.find(\"ul\", class_=\"jobsearch-ResultsList\")\n",
    "\n",
    "    jobs = job_list.find_all('li', recursive=False)\n",
    "\n",
    "    for job in jobs:\n",
    "        zone = job.find(\"div\", class_=\"mosaic-zone\")\n",
    "        if zone == None: # mosaic-zone을 찾지 못하면, None 리턴\n",
    "            anchor = job.select_one(\"h2 a\")\n",
    "            title = anchor['aria-label']\n",
    "            link = anchor['href']\n",
    "            company = job.find(\"span\", class_=\"companyName\")\n",
    "            location = job.find(\"div\", class_=\"companyLocation\")\n",
    "            job_data = {\n",
    "                'link': f\"https://kr.indeed.com{link}\",\n",
    "                'company': company.string,\n",
    "                'location': location.string,\n",
    "                'position': title\n",
    "            }\n",
    "            results.append(job_data)\n",
    "    for result in results:\n",
    "        print(result, \"\\n///////\\n\")        \n",
    "\n",
    "# 코드 무한대기: 크롬 드라이버 버전 안맞아서 생기는 문제!!\n",
    "while (True):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.16 Pages Part Two\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# options = Options()\n",
    "# options.add_argument(\"--no-sandbox\")\n",
    "# options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# browser = webdriver.Chrome(options=options)\n",
    "\n",
    "def get_page_count(keyword):\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(f\"https://kr.indeed.com/jobs?q={keyword}\")\n",
    "    soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "    pagination = soup.find(\"ul\", class_=\"pagination-list\") # 첫 페이지 확인\n",
    "    if pagination == None: # pagination = None이면 스크래핑할 페이지 1개\n",
    "        return 1\n",
    "    pages = pagination.find_all(\"li\", recursive=False) # 페이지 1, 2, 3, 4, 5, 다음 -> len = 6\n",
    "    count = len(pages)\n",
    "    if count >= 5:\n",
    "        return 5\n",
    "    else:\n",
    "        return count\n",
    "\n",
    "\n",
    "def extract_indeed_jobs(keyword):\n",
    "    pages = get_page_count(keyword)\n",
    "    for page in range(pages):\n",
    "        browser = webdriver.Chrome()\n",
    "        browser.get(f\"https://kr.indeed.com/jobs?q={keyword}\")\n",
    "        results = []\n",
    "        soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "        job_list = soup.find(\"ul\", class_=\"jobsearch-ResultsList\")\n",
    "\n",
    "        jobs = job_list.find_all('li', recursive=False)\n",
    "\n",
    "        for job in jobs:\n",
    "            zone = job.find(\"div\", class_=\"mosaic-zone\")\n",
    "            if zone == None: # mosaic-zone을 찾지 못하면, None 리턴\n",
    "                anchor = job.select_one(\"h2 a\")\n",
    "                title = anchor['aria-label']\n",
    "                link = anchor['href']\n",
    "                company = job.find(\"span\", class_=\"companyName\")\n",
    "                location = job.find(\"div\", class_=\"companyLocation\")\n",
    "                job_data = {\n",
    "                    'link': f\"https://kr.indeed.com{link}\",\n",
    "                    'company': company.string,\n",
    "                    'location': location.string,\n",
    "                    'position': title\n",
    "                }\n",
    "                results.append(job_data)\n",
    "   \n",
    "# 코드 무한대기: 크롬 드라이버 버전 안맞아서 생기는 문제!!\n",
    "while (True):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.17 Refactor\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from extractors.wwr import extract_wwr_jobs\n",
    "# bot 인줄 알고 차단당해서 selenium 써야함\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "def get_page_count(keyword):\n",
    "    options = Options()\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    browser = webdriver.Chrome(options = options)\n",
    "    base_url = \"https://kr.indeed.com/jobs?q=\"\n",
    "    browser.get(f\"{base_url}{keyword}\")\n",
    "    soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "    # nav로 HTML 바꼇음, + aria-label 추가\n",
    "    pagination = soup.find(\"nav\", {\"aria-label\":\"pagination\"})\n",
    "    if pagination == None:\n",
    "        return 1\n",
    "    else:\n",
    "        pages = pagination.find_all(\"div\", recursive = False)\n",
    "        count = len(pages)\n",
    "    if count >= 5:\n",
    "        return 5\n",
    "    else:\n",
    "        return count\n",
    "\n",
    "def extract_indeed_jobs(keyword):\n",
    "    pages = get_page_count(keyword)\n",
    "    print(\"found\", pages)\n",
    "    # page 는 0 부터 시작\n",
    "    results = []\n",
    "    for page in range(pages):\n",
    "        # Replit 옵션, VSC 에서는 필요 X\n",
    "        options = Options()\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        browser = webdriver.Chrome(options = options)\n",
    "\n",
    "        base_url = \"https://kr.indeed.com/jobs\"\n",
    "        final_url = f\"{base_url}?q={keyword}&start={page*10}\"\n",
    "        browser.get(final_url)\n",
    "        print(final_url)\n",
    "\n",
    "        soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "        job_lists = soup.find(\"ul\", class_ = \"jobsearch-ResultsList\")\n",
    "        jobs = job_lists.find_all(\"li\", recursive = False)\n",
    "        # recursive 를 False 로 하면 아랫단계까지 찾는게 아니라 바로 아래 단계에서만 찾는다\n",
    "        for job in jobs:\n",
    "            zone = job.find(\"div\", class_ = \"mosaic-zone\")\n",
    "            if zone == None: # None 은 없음을 의미, False 랑은 다름\n",
    "                #anchor = job.select(\"h2 a\") #h2 안에 있는 a 를 가져오기\n",
    "                anchor = job.select_one(\"h2 a\")\n",
    "                title = anchor['aria-label']\n",
    "                link = anchor['href']\n",
    "                company = job.find(\"span\", class_ = \"companyName\")\n",
    "                region = job.find(\"div\", class_ = \"companyLocation\")\n",
    "            job_data = {\n",
    "                \"link\": f\"https://kr.indeed.com/{link}\",\n",
    "                \"company\": company.string,\n",
    "                \"location\": region.sting,\n",
    "                \"position\": title\n",
    "                }\n",
    "            results.append(job_data)\n",
    "    return results\n",
    "\n",
    "num = get_page_count(\"react\")\n",
    "#result = extract_indeed_jobs(\"python\")\n",
    "print(num)\n",
    "print(extract_indeed_jobs(\"react\"))\n",
    "\n",
    "# 코드 무한대기: 크롬 드라이버 버전 안맞아서 생기는 문제!!\n",
    "#while (True):\n",
    "#    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.18 Recap\n",
    "\n",
    "from extractors.indeed import extract_indeed_jobs\n",
    "from extractors.wwr import extract_wwr_jobs\n",
    "\n",
    "keyword = input(\"What do you want to search for?\")\n",
    "\n",
    "# indeed, wwr: list 로 리턴\n",
    "indeed = extract_indeed_jobs(keyword)\n",
    "wwr = extract_wwr_jobs(keyword)\n",
    "\n",
    "jobs = indeed + wwr\n",
    "\n",
    "for job in jobs:\n",
    "    print(job)\n",
    "    print(\"/////\\n/////\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.19 Write to File\n",
    "\n",
    "from extractors.indeed import extract_indeed_jobs\n",
    "from extractors.wwr import extract_wwr_jobs\n",
    "\n",
    "keyword = input(\"What do you want to search for?\")\n",
    "\n",
    "# indeed, wwr: list 로 리턴\n",
    "indeed = extract_indeed_jobs(keyword)\n",
    "wwr = extract_wwr_jobs(keyword)\n",
    "\n",
    "jobs = indeed + wwr\n",
    "\n",
    "file = open(f\"{keyword}.csv\", \"w\") # csv 파일 생성\n",
    "file.write(\"Position, Company, Location, URL\\n\") # Column 이름\n",
    "\n",
    "for job in jobs: # 각 dict 불러와 csv에 작성\n",
    "    file.write(f\"{job['position']}, {job['company']}, {job['location']}, {job['link']}\\n\")\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Hello Flask\n",
    "\n",
    "from flask import Flask\n",
    "\n",
    "app = Flask(\"JobScrapper\")\n",
    "\n",
    "\n",
    "@app.route(\"/\") # 이 위치로 이동시, 아래 함수 실행\n",
    "def home():\n",
    "    return \"hey there!\"\n",
    "\n",
    "\n",
    "app.run(\"0.0.0.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Render Template\n",
    "\n",
    "from flask import Flask\n",
    "\n",
    "app = Flask(\"JobScrapper\")\n",
    "\n",
    "\n",
    "@app.route(\"/\") # 이 위치로 이동시, 아래 함수 실행\n",
    "def home():\n",
    "    return \"<h1>hey there!</h1>\"\n",
    "\n",
    "@app.route(\"/hello\")\n",
    "def hello():\n",
    "    return 'hello you!'\n",
    "\n",
    "app.run(\"0.0.0.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.5 Arguments\n",
    "\n",
    "from flask import Flask, render_template, request\n",
    "from extractors.indeed import extract_indeed_jobs\n",
    "from extractors.wwr import extract_wwr_jobs\n",
    "\n",
    "app = Flask(\"JobScrapper\")\n",
    "\n",
    "\n",
    "@app.route(\"/\") # 이 위치로 이동시, 아래 함수 실행\n",
    "def home():\n",
    "    return render_template(\"home.html\", name=\"nico\")\n",
    "\n",
    "@app.route(\"/search\")\n",
    "def search():\n",
    "    keyword = request.args.get(\"keyword\")\n",
    "    indeed = extract_indeed_jobs(keyword)\n",
    "    wwr = extract_wwr_jobs(keyword)\n",
    "    jobs = indeed + wwr\n",
    "    return render_template(\"search.html\", keyword=keyword, jobs=jobs)\n",
    "\n",
    "app.run(\"0.0.0.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.7 Pico\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Job Scrapper</title>\n",
    "    <link rel=\"stylesheet\" href=\"https://unpkg.com/@picocss/pico@latest/css/pico.min.css\">\n",
    "</head>\n",
    "<body>\n",
    "    <main class=\"container\">\n",
    "        <h1>Search Result for \"{{keyword}}\":</h1>\n",
    "        <figure>\n",
    "            <table role=\"grid\">\n",
    "                <thead>\n",
    "                    <tr>\n",
    "                        <th>Position</th>\n",
    "                        <th>Company</th>\n",
    "                        <th>Location</th>\n",
    "                        <th>Link</th>\n",
    "                    </tr>\n",
    "                </thead>\n",
    "                <tbody>\n",
    "                    {% for job in jobs %}\n",
    "                    <tr>\n",
    "                        <td>{{job.position}}</td>\n",
    "                        <td>{{job.company}}</td>\n",
    "                        <td>{{job.location}}</td>\n",
    "                        <td><a href=\"{{job.link}}\" target=\"_blank\">Apply &rarr;</a></td>\n",
    "                    </tr>\n",
    "                    {% endfor %}\n",
    "                </tbody>\n",
    "            </table>\n",
    "        </figure>\n",
    "    </main>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.8 Cache\n",
    "\n",
    "from flask import Flask, render_template, request\n",
    "from extractors.indeed import extract_indeed_jobs\n",
    "from extractors.wwr import extract_wwr_jobs\n",
    "\n",
    "app = Flask(\"JobScrapper\")\n",
    "\n",
    "\n",
    "@app.route(\"/\") # 이 위치로 이동시, 아래 함수 실행\n",
    "def home():\n",
    "    return render_template(\"home.html\", name=\"nico\")\n",
    "\n",
    "db = {} # 가짜 DB, Cache\n",
    "\n",
    "@app.route(\"/search\")\n",
    "def search():\n",
    "    keyword = request.args.get(\"keyword\")\n",
    "    if keyword in db:\n",
    "        jobs = db[keyword]\n",
    "    else:\n",
    "        indeed = extract_indeed_jobs(keyword)\n",
    "        wwr = extract_wwr_jobs(keyword)\n",
    "        jobs = indeed + wwr\n",
    "        db[keyword] = jobs\n",
    "    return render_template(\"search.html\", keyword=keyword, jobs=jobs)\n",
    "\n",
    "app.run(\"0.0.0.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.9 File Download\n",
    "\n",
    "from flask import Flask, render_template, request, redirect, send_file\n",
    "from extractors.indeed import extract_indeed_jobs\n",
    "from extractors.wwr import extract_wwr_jobs\n",
    "from file import save_to_file\n",
    "\n",
    "app = Flask(\"JobScrapper\")\n",
    "\n",
    "\n",
    "@app.route(\"/\") # 이 위치로 이동시, 아래 함수 실행\n",
    "def home():\n",
    "    return render_template(\"home.html\", name=\"nico\")\n",
    "\n",
    "db = {} # 가짜 DB, Cache\n",
    "\n",
    "@app.route(\"/search\")\n",
    "def search():\n",
    "    keyword = request.args.get(\"keyword\")\n",
    "    if keyword == None:\n",
    "        return redirect(\"/\")\n",
    "    if keyword in db:\n",
    "        jobs = db[keyword]\n",
    "    else:\n",
    "        indeed = extract_indeed_jobs(keyword)\n",
    "        wwr = extract_wwr_jobs(keyword)\n",
    "        jobs = indeed + wwr\n",
    "        db[keyword] = jobs\n",
    "    return render_template(\"search.html\", keyword=keyword, jobs=jobs)\n",
    "\n",
    "@app.route(\"/export\")\n",
    "def export():\n",
    "    keyword = request.args.get(\"keyword\")\n",
    "    if keyword == None:\n",
    "        return redirect(\"/\")\n",
    "    if keyword not in db:\n",
    "        return redirect(f\"/search?keyword={keyword}\")\n",
    "    save_to_file(keyword, db[keyword])\n",
    "    return send_file(f\"{keyword}.csv\", as_attachment=True)\n",
    "\n",
    "app.run(\"0.0.0.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
